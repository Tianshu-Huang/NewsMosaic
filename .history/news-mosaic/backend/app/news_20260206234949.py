import httpx
import hashlib
import json
from datetime import datetime, timedelta
from .models import Article
from .settings import (
    NEWS_API_KEY,
    GUARDIAN_API_KEY,
    NEWSDATA_API_KEY,
    REDDIT_CLIENT_ID,
    REDDIT_CLIENT_SECRET,
    ENABLE_NEWS_API,
    ENABLE_GUARDIAN,
    ENABLE_NEWSDATA,
    ENABLE_REDDIT,
    ENABLE_HACKERNEWS,
)
from .sample_data import SAMPLE_ARTICLES

def _make_id(title: str, source: str, published_at: str) -> str:
    raw = f"{title}|{source}|{published_at}".encode("utf-8")
    return hashlib.sha256(raw).hexdigest()[:16]

def _has_real_newsapi_key() -> bool:
    if not NEWS_API_KEY:
        return False
    k = NEWS_API_KEY.strip()
    if k.lower() in {"xxxx", "your_key_here", "replace_me"}:
        return False
    return True

# ==================== NewsAPI ====================
async def fetch_from_newsapi(query: str, max_articles: int) -> list[Article]:
    """ä» NewsAPI è·å–æ–‡ç« """
    if not ENABLE_NEWS_API or not _has_real_newsapi_key():
        return []

    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "pageSize": min(max_articles, 100),
        "sortBy": "publishedAt",
        "language": "en",
        "apiKey": NEWS_API_KEY,
    }

    try:
        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(url, params=params)
            r.raise_for_status()
            data = r.json()

        articles: list[Article] = []
        for a in data.get("articles", []):
            title = a.get("title") or ""
            if not title.strip():
                continue
            source = (a.get("source") or {}).get("name", "") or "NewsAPI"
            published_at = a.get("publishedAt", "") or ""
            snippet = a.get("description", "") or ""
            link = a.get("url", "") or ""
            if not link:
                continue
            articles.append(
                Article(
                    id=_make_id(title, source, published_at),
                    title=title,
                    snippet=snippet,
                    source=source,
                    published_at=published_at,
                    url=link,
                )
            )
        return articles
    except Exception as e:
        print(f"âŒ NewsAPI é”™è¯¯: {e}")
        return []


# ==================== The Guardian ====================
async def fetch_from_guardian(query: str, max_articles: int) -> list[Article]:
    """ä» The Guardian API è·å–æ–‡ç« """
    if not ENABLE_GUARDIAN:
        return []

    url = "https://open.theguardian.com/api/search"
    params = {
        "q": query,
        "page-size": min(max_articles, 200),
        "order-by": "newest",
        "api-key": GUARDIAN_API_KEY,
    }

    try:
        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(url, params=params)
            r.raise_for_status()
            data = r.json()

        articles: list[Article] = []
        for item in data.get("response", {}).get("results", []):
            title = item.get("webTitle", "") or ""
            if not title.strip():
                continue
            published_at = item.get("webPublicationDate", "") or ""
            link = item.get("webUrl", "") or ""
            if not link:
                continue
            snippet = item.get("fields", {}).get("trailText", "") or ""
            articles.append(
                Article(
                    id=_make_id(title, "The Guardian", published_at),
                    title=title,
                    snippet=snippet,
                    source="The Guardian",
                    published_at=published_at,
                    url=link,
                )
            )
        return articles
    except Exception as e:
        print(f"âŒ The Guardian é”™è¯¯: {e}")
        return []


# ==================== NewsData.io ====================
async def fetch_from_newsdata(query: str, max_articles: int) -> list[Article]:
    """ä» NewsData.io è·å–æ–‡ç« """
    if not ENABLE_NEWSDATA:
        return []

    url = "https://newsdata.io/api/1/news"
    params = {
        "q": query,
        "pagesize": min(max_articles, 50),
        "language": "en",
        "apikey": NEWSDATA_API_KEY,
    }

    try:
        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(url, params=params)
            r.raise_for_status()
            data = r.json()

        articles: list[Article] = []
        for item in data.get("results", []):
            title = item.get("title", "") or ""
            if not title.strip():
                continue
            source = item.get("source_id", "") or "NewsData"
            published_at = item.get("pubDate", "") or ""
            link = item.get("link", "") or ""
            if not link:
                continue
            snippet = item.get("description", "") or ""
            articles.append(
                Article(
                    id=_make_id(title, source, published_at),
                    title=title,
                    snippet=snippet,
                    source=source,
                    published_at=published_at,
                    url=link,
                )
            )
        return articles
    except Exception as e:
        print(f"âŒ NewsData.io é”™è¯¯: {e}")
        return []


# ==================== Reddit ====================
async def fetch_from_reddit(query: str, max_articles: int) -> list[Article]:
    """ä» Reddit è·å–æ–‡ç« ï¼ˆæ— éœ€è®¤è¯ï¼Œä½¿ç”¨å…¬å¼€ JSON endpointï¼‰"""
    if not ENABLE_REDDIT:
        return []

    try:
        # Reddit æ”¯æŒåœ¨ URL ååŠ  .json è·å–å…¬å¼€æ•°æ®ï¼Œæ— éœ€è®¤è¯
        # ä½¿ç”¨ old.reddit.com çš„ search.json ç«¯ç‚¹
        url = "https://old.reddit.com/r/news/search.json"
        params = {
            "q": query,
            "limit": min(max_articles, 100),
            "sort": "new",
        }
        headers = {"User-Agent": "NewsMosaic/1.0"}

        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(url, params=params, headers=headers)
            r.raise_for_status()
            data = r.json()

        articles: list[Article] = []
        for item in data.get("data", {}).get("children", []):
            post = item.get("data", {})
            title = post.get("title", "") or ""
            if not title.strip():
                continue
            # Reddit å¸–å­çš„å‘å¸ƒæ—¶é—´
            published_at = datetime.fromtimestamp(
                post.get("created_utc", 0)
            ).isoformat()
            link = f"https://reddit.com{post.get('permalink', '')}"
            snippet = post.get("selftext", "")[:500] or ""
            source = f"r/{post.get('subreddit', 'reddit')}"

            articles.append(
                Article(
                    id=_make_id(title, source, published_at),
                    title=title,
                    snippet=snippet,
                    source=source,
                    published_at=published_at,
                    url=link,
                )
            )
        return articles
    except Exception as e:
        print(f"âŒ Reddit é”™è¯¯: {e}")
        return []


# ==================== HackerNews ====================
async def fetch_from_hackernews(query: str, max_articles: int) -> list[Article]:
    """ä» HackerNews (Algolia) è·å–æ–‡ç« """
    if not ENABLE_HACKERNEWS:
        return []

    url = "https://hn.algolia.com/api/v1/search"
    params = {
        "query": query,
        "hitsPerPage": min(max_articles, 100),
        "numericFilters": "created_at_i>0",  # æœ€è¿‘çš„æ–‡ç« 
    }

    try:
        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(url, params=params)
            r.raise_for_status()
            data = r.json()

        articles: list[Article] = []
        for hit in data.get("hits", []):
            # åªè·å–æœ‰æ ‡é¢˜å’Œ URL çš„æ–‡ç« 
            if hit.get("story_title"):
                title = hit.get("story_title", "")
            elif hit.get("title"):
                title = hit.get("title", "")
            else:
                continue

            if not title.strip():
                continue

            url = hit.get("story_url") or hit.get("url") or ""
            if not url:
                url = f"https://news.ycombinator.com/item?id={hit.get('objectID', '')}"

            published_at = hit.get("created_at", "") or ""
            snippet = hit.get("story_text", "") or ""

            articles.append(
                Article(
                    id=_make_id(title, "HackerNews", published_at),
                    title=title,
                    snippet=snippet[:500] if snippet else "",
                    source="HackerNews",
                    published_at=published_at,
                    url=url,
                )
            )
        return articles
    except Exception as e:
        print(f"âŒ HackerNews é”™è¯¯: {e}")
        return []


# ==================== ä¸»å‡½æ•° - èšåˆæ‰€æœ‰æ¥æº ====================
async def fetch_news(query: str, days: int, max_articles: int) -> list[Article]:
    """ä»å¤šä¸ªæ¥æºèšåˆæ–°é—»"""
    
    # âœ… å¦‚æœæ²¡æœ‰çœŸå® API keysï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®
    if not (ENABLE_NEWS_API or ENABLE_GUARDIAN or ENABLE_NEWSDATA or ENABLE_REDDIT or ENABLE_HACKERNEWS):
        print("âš ï¸  æœªé…ç½®ä»»ä½•æ–°é—»æ¥æºï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        return SAMPLE_ARTICLES[:max_articles]

    print(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
    print(f"ğŸ“¡ å¯ç”¨çš„æ¥æº: NewsAPI={ENABLE_NEWS_API}, Guardian={ENABLE_GUARDIAN}, "
          f"NewsData={ENABLE_NEWSDATA}, Reddit={ENABLE_REDDIT}, HackerNews={ENABLE_HACKERNEWS}")

    # å¹¶å‘è·å–æ‰€æœ‰æ¥æºçš„æ•°æ®
    tasks = []
    
    if ENABLE_NEWS_API and _has_real_newsapi_key():
        tasks.append(fetch_from_newsapi(query, max_articles))
    if ENABLE_GUARDIAN:
        tasks.append(fetch_from_guardian(query, max_articles))
    if ENABLE_NEWSDATA:
        tasks.append(fetch_from_newsdata(query, max_articles))
    if ENABLE_REDDIT:
        tasks.append(fetch_from_reddit(query, max_articles))
    if ENABLE_HACKERNEWS:
        tasks.append(fetch_from_hackernews(query, max_articles))

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
    import asyncio
    results = await asyncio.gather(*tasks)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_articles: list[Article] = []
    for articles in results:
        all_articles.extend(articles)

    # å»é‡ï¼ˆæŒ‰ titleï¼‰
    seen = set()
    deduped = []
    for art in all_articles:
        key = art.title.strip().lower()
        if key in seen:
            continue
        seen.add(key)
        deduped.append(art)

    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    deduped.sort(
        key=lambda x: x.published_at if x.published_at else "",
        reverse=True
    )

    result = deduped[:max_articles]
    print(f"âœ… å…±è·å– {len(result)} ç¯‡æ–‡ç« ä» {len([r for r in results if r])} ä¸ªæ¥æº")
    
    return result
